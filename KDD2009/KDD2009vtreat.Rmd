
[KDD2009 example](http://www.sigkdd.org/kdd-cup-2009-customer-relationship-prediction).  Winners had hold-out AUC of 0.7611 on churn.   See [here](https://github.com/WinVector/zmPDSwR/tree/master/KDD2009) for more details.

```{r kddexlibs, tidy=FALSE}
#load some libraries
# http://www.win-vector.com/blog/2014/08/vtreat-designing-a-package-for-variable-treatment/
library('vtreat')
packageVersion('vtreat')
# devtools::install_github("WinVector/WVPlots")
library('WVPlots')

library('parallel')
library('gbm')
#library('class')
library('ggplot2')
library('glmnet')
library('xgboost')


# load the data as in the book
# change this path to match your directory structure
dir = '~/Documents/work/PracticalDataScienceWithR/zmPDSwR/KDD2009/' 

d = read.table(paste(dir,'orange_small_train.data.gz',sep=''),
                header=T,sep='\t',na.strings=c('NA',''), 
               stringsAsFactors=FALSE)
churn = read.table(paste(dir,'orange_small_train_churn.labels.txt',sep=''),
                    header=F,sep='\t')
d$churn = churn$V1
appetency = read.table(paste(dir,'orange_small_train_appetency.labels.txt',sep=''),
                        header=F,sep='\t')
d$appetency = appetency$V1
upselling = read.table(paste(dir,'orange_small_train_upselling.labels.txt',sep=''),
                        header=F,sep='\t')
d$upselling = upselling$V1
set.seed(729375)
rgroup = runif(dim(d)[[1]])
dTrain = d[rgroup<=0.9,]  # set for building models
dTest = d[rgroup>0.9,] # set for evaluation
debug = FALSE
if(debug) {
  dTrain <- dTrain[sample.int(nrow(dTrain),100),]
  dTest <- dTest[sample.int(nrow(dTest),100),]
}
rm(list=c('d','churn','appetency','upselling','dir'))
outcomes = c('churn','appetency','upselling')
nonvars <- c(outcomes,'rgroup')
vars = setdiff(colnames(dTrain),
                nonvars)
yName = 'churn'
yTarget = 1
```



```{r kdddesign, tidy=FALSE}
# build data treatments

set.seed(239525)

cl <- c()
if(!debug) {
  ncore <- parallel::detectCores()
  cl <- parallel::makeCluster(ncore)
}

# build treatments 
trainPlan = mkCrossFrameCExperiment(dTrain,
    vars,yName,yTarget,
    smFactor=2.0, 
    parallelCluster=cl)
print(trainPlan$method)
treatmentsC = trainPlan$treatments
treatedTrainM = trainPlan$crossFrame

kddSig = 1/nrow(treatmentsC$scoreFrame)
print(kddSig)
selvars = treatmentsC$scoreFrame$varName[treatmentsC$scoreFrame$sig<kddSig]
treatedTrainM[[yName]] = treatedTrainM[[yName]]==yTarget

treatedTest = prepare(treatmentsC,
                      dTest,
                      pruneSig=kddSig, 
                      parallelCluster=cl)
treatedTest[[yName]] = treatedTest[[yName]]==yTarget


if(!is.null(cl)) {
    parallel::stopCluster(cl)
    cl = NULL
}
```





```{r kddmodels, tidy=FALSE}
# Run other models (with proper coding/training separation).
#
# This gets us back to AUC 0.72

print(selvars)

# prepare plotting frames
treatedTrainP = treatedTrainM[, yName, drop=FALSE]
treatedTestP = treatedTest[, yName, drop=FALSE]


formulaS = paste(yName,paste(selvars,collapse=' + '),sep=' ~ ')
for(mname in c('glmPred','gbmPred','xgboost')) {
  print("*****************************")
  print(date())
  print(paste(mname,length(selvars)))
  if(mname=='gbmPred') {
    modelGBMs = gbm(as.formula(formulaS),
                    data=treatedTrainM,
                    distribution='bernoulli',
                    n.trees=1000,
                    interaction.depth=3,
                    keep.data=FALSE,
                    cv.folds=5)
    #print(modelGBMs)
    #print(summary(modelGBMs))
    nTrees = gbm.perf(modelGBMs)
    treatedTrainP[[mname]] = predict(modelGBMs,
                                    newdata=treatedTrainM,type='response',
                                    n.trees=nTrees)
    treatedTestP[[mname]] = predict(modelGBMs,
                                    newdata=treatedTest,type='response',
                                    n.trees=nTrees)
  } else if(mname=='glmPred') {
    modelglms = cv.glmnet(x = as.matrix(treatedTrainM[,selvars,drop=FALSE]),
                          y = treatedTrainM[[yName]],
                          alpha=0.5,
                          family='binomial')
    #print(summary(modelglms))
    treatedTrainP[[mname]] = as.numeric(predict(modelglms,
                                               newx=as.matrix(treatedTrainM[,selvars,drop=FALSE]),
                                               type='response'))
    treatedTestP[[mname]] = as.numeric(predict(modelglms,
                                               newx=as.matrix(treatedTest[,selvars,drop=FALSE]),
                                               type='response'))
  } else if(mname=='xgboost') {
    modelxg = xgboost(data=xgb.DMatrix(as.matrix(treatedTrainM[,selvars,drop=FALSE]),
                                       label=treatedTrainM[[yName]]),
                      objective='binary:logistic', 
                      nrounds=100,
                      nthread=parallel::detectCores())
    treatedTrainP[[mname]] = as.numeric(predict(modelxg,
                                               as.matrix(treatedTrainM[,selvars,drop=FALSE])))
    treatedTestP[[mname]] = as.numeric(predict(modelxg,
                                               as.matrix(treatedTest[,selvars,drop=FALSE])))
  }
  
  t1 = paste(mname,'train data')
  print(DoubleDensityPlot(treatedTrainP, mname, yName, 
                          title=t1))
  print(ROCPlot(treatedTrainP, mname, yName, 
                title=t1))
  
  t2 = paste(mname,'test data')
  print(DoubleDensityPlot(treatedTestP, mname, yName, 
                          title=t2))
  print(ROCPlot(treatedTestP, mname, yName, 
                title=t2))
  
  print(date())
  print("*****************************")
}

```
